{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import simpy\n",
    "import math\n",
    "import scipy.stats as st\n",
    "from probabilities import *\n",
    "from collections import Counter\n",
    "\n",
    "twitter_datas = pd.read_csv(\"twitter.csv\")\n",
    "twitter_datas.columns = ['Time', 'QPS']\n",
    "perform_datas = pd.read_csv(\"perform.csv\")\n",
    "# pd.options.display.float_format = '{:.10f}'.format\n",
    "\n",
    "\n",
    "lambda_data_2048 = {\n",
    "    'mobilenet_v2': 43.6,\n",
    "    'inception_v3': 275.5,\n",
    "    'resnet50' : 193,\n",
    "    'vgg16': 466,\n",
    "    'vgg19': 554,\n",
    "}\n",
    "lambda_price_2048 = 0.0000000333\n",
    "\n",
    "print(perform_datas)\n",
    "\n",
    "inferentia_price = 0.0060333333 / 60\n",
    "print(inferentia_price)\n",
    "\n",
    "datas = twitter_datas\n",
    "\n",
    "x = list(datas.get('Time').values)\n",
    "y = list(datas.get('QPS').values)\n",
    "\n",
    "print(datas['QPS'][:600].sum())\n",
    "\n",
    "totals = datas['QPS'][:600].sum()\n",
    "\n",
    "TARGET_LATENCY = 200 #ms\n",
    "T = [1,5,10,30,60,180]\n",
    "\n",
    "default = {}\n",
    "for t in T:\n",
    "    default[t] = 0\n",
    "\n",
    "inferentia_price = 0.000100555555\n",
    "model = \"resnet50\"\n",
    "\n",
    "model_index = {\n",
    "    \"mobilenet_v2\":0,\n",
    "    \"inception_v3\":1,\n",
    "    \"resnet50\":2,\n",
    "    \"vgg16\":3,\n",
    "    \"vgg19\":4\n",
    "}\n",
    "\n",
    "df = perform_datas[perform_datas[\"models\"].isin([model])]\n",
    "inferentia_per_second = int(df['InferentiaPerform'].values[0] / 60)\n",
    "lambda_per_second = int(df['LambdaPerform'].values[0] / 60)\n",
    "\n",
    "lambda_per_price = float(df['LambdaEventPerPrice'].values[0])\n",
    "\n",
    "PerformInferentia = copy.deepcopy(default)\n",
    "for t in T:\n",
    "    PerformInferentia[t] = inferentia_per_second * t\n",
    "\n",
    "PerformLambda = copy.deepcopy(default)\n",
    "for t in T:\n",
    "    PerformLambda[t] = lambda_per_second * t\n",
    "\n",
    "# print(\"Perform_Inferentia:\", PerformInferentia)\n",
    "# print(\"Perform_Lambda:\", PerformLambda)\n",
    "\n",
    "InferentiaInstances = 0\n",
    "InferentiaJobs = 0\n",
    "LambdaWorkers = 0\n",
    "\n",
    "Instance_Cold_Start = 10\n",
    "EVENT_TESTING = 10\n",
    "\n",
    "WorkedByLambda = []\n",
    "WorkedByInferentia = []\n",
    "\n",
    "# Time Step 별 도착하는 Event 양 체크\n",
    "def RequestMonitor(start_time):\n",
    "    requests = copy.deepcopy(default)\n",
    "    for t in T:\n",
    "        # 현재 시간 부터 앞으로의 t 만큼 체크, 이벤트의 총합 계산\n",
    "        requests[t] = twitter_datas[start_time:start_time + t]['QPS'].values.sum(axis=0) * EVENT_TESTING\n",
    "    return requests\n",
    "\n",
    "start_time = 0\n",
    "end_time = 599\n",
    "\n",
    "instance_start_time = 0\n",
    "SIMULATIONS = 10\n",
    "InstanceOnTimes = 0\n",
    "MaxInstances = 0\n",
    "\n",
    "while(start_time <= end_time):\n",
    "#     print(\"Current Time:\", start_time)\n",
    "\n",
    "    Events = RequestMonitor(start_time)\n",
    "#     print(Events)\n",
    "\n",
    "    ComparedEventValues = TotalEventValues = np.array(list(Events.values()))\n",
    "\n",
    "    CurrentLambdaJobs = 0\n",
    "    CurrentInferentiaJobs = 0\n",
    "\n",
    "    ### SETTINGS\n",
    "    # Set RHO to a little bit smaller then 1; makes the simulation interesting\n",
    "    # RHO = 서버 활용도\n",
    "    # MU > LAMBDA, if  mu = 1 and c is 1 otherwise no queue.\n",
    "    # 1/MU > 1/LAMBDA if c=2 or higher?\n",
    "    # If mu = 2, avg is every 0.5 time step is the time costs of a service.\n",
    "    # suppose lambda < 1\n",
    "\n",
    "    NeedInstances = 0\n",
    "    while True:\n",
    "        RemainEvents = TotalEventValues - np.array(list(PerformInferentia.values())) * (NeedInstances + 1)\n",
    "        EventRatio = len(RemainEvents[RemainEvents < 0]) / len(RemainEvents)\n",
    "        if EventRatio > 0.5:\n",
    "            break\n",
    "        else:\n",
    "            NeedInstances +=1\n",
    "\n",
    "    InferentiaInstances = NeedInstances\n",
    "\n",
    "\n",
    "\n",
    "    if InferentiaInstances == 0:\n",
    "        RemainEvents = TotalEventValues\n",
    "    else:\n",
    "        RemainEvents = TotalEventValues % (InferentiaInstances * np.array(list(PerformInferentia.values())))\n",
    "\n",
    "    PreferedLambda = np.array(list(PerformLambda.values())) > RemainEvents\n",
    "    LambdaRatio = len(PreferedLambda[PreferedLambda == True]) / len(PreferedLambda)\n",
    "#     print(\"LambdaRatio:\", LambdaRatio)\n",
    "\n",
    "    LambdaUsed = False\n",
    "    if LambdaRatio <= 0.5:\n",
    "        NeedInstances +=1\n",
    "    else:\n",
    "        LambdaUsed = True\n",
    "\n",
    "\n",
    "\n",
    "    CurrentInferentiaJobs = Events[1]\n",
    "    InferentiaInstances = NeedInstances\n",
    "\n",
    "    if InferentiaInstances > 0:\n",
    "#         print('Servers:', InferentiaInstances)\n",
    "        # SIM_TIME: simulation time in time units\n",
    "        time_idx =0\n",
    "        for TimeKey, SIM_TIME in PerformInferentia.items():\n",
    "\n",
    "#             print(SIM_TIME)\n",
    "#             print(TimeKey)\n",
    "#             print(1/MU)\n",
    "            SERVERS = InferentiaInstances\n",
    "            MU = SIM_TIME / TimeKey # 1/mu is exponential service times\n",
    "            LAMBDA = TotalEventValues[time_idx] / TimeKey\n",
    "            RHO = LAMBDA / (MU * SERVERS)\n",
    "\n",
    "#             print(\"RHO:\",RHO)\n",
    "#             print(expw(MU, SERVERS, RHO) / 2)\n",
    "            W = expw(MU, SERVERS, RHO) / 2 + 1/MU\n",
    "#             print(\"SIM_TIME:\",SIM_TIME)\n",
    "#             print(\"EXPECTED VALUES AND PROBABILITIES\")\n",
    "\n",
    "#             print(f'Rho: {RHO}\\nMu: {MU}\\nLambda: {LAMBDA}\\nExpected interarrival time: {1 / LAMBDA:.5f} time units')\n",
    "#             print(f'Expected processing time per server: {1 / MU:.5f} time units\\n')\n",
    "#             print(f'Probability that a job has to wait: {pwait(SERVERS, RHO):.5f}')\n",
    "#             print(f'Expected queue length E(Lq): {expquel(SERVERS, RHO):.5f} customers\\n')\n",
    "#             print(f'Expected waiting time E(W): {W:.5f} time units\\n')\n",
    "#             E = expw(MU, SERVERS, RHO)\n",
    "\n",
    "            time_idx +=1\n",
    "\n",
    "#             if W > TARGET_LATENCY / 1000:\n",
    "#                 LambdaUsed = True\n",
    "#                 CurrentLambdaJobs += int(TotalEventValues[0] / 2)\n",
    "#                 print(\"W:\", W, \"Violate Target Latency\")\n",
    "\n",
    "    if LambdaUsed:\n",
    "        CurrentLambdaJobs += RemainEvents[0]\n",
    "        LambdaWorkers += CurrentLambdaJobs\n",
    "#     print(Events)\n",
    "#     print(RemainEvents[0])\n",
    "#     print(InferentiaInstances)\n",
    "#     print(CurrentInferentiaJobs)\n",
    "    CurrentInferentiaJobs = Events[1] - CurrentLambdaJobs\n",
    "    InferentiaJobs += CurrentInferentiaJobs\n",
    "\n",
    "    WorkedByLambda.append(CurrentLambdaJobs)\n",
    "    WorkedByInferentia.append(CurrentInferentiaJobs)\n",
    "    InstanceOnTimes += 1 * InferentiaInstances\n",
    "#     print(\"Lambda_Workers:\",LambdaWorkers)\n",
    "#     print(\"InferentiaInstances:\",InferentiaInstances, \"Worked by Inferentia Job:\", InferentiaJobs)\n",
    "\n",
    "    if MaxInstances < InferentiaInstances:\n",
    "        MaxInstances = InferentiaInstances\n",
    "    start_time += 1\n",
    "    #if real simulation\n",
    "    #time.sleep(1)\n",
    "print(\"Model Name:\",model)\n",
    "print(\"Lambda_Workers:\",LambdaWorkers)\n",
    "print(\"InferentiaInstances:\",InferentiaInstances, \"Worked by Inferentia Job:\", InferentiaJobs)\n",
    "print(\"InstanceOnTimes:\", InstanceOnTimes)\n",
    "print(\"Instance Prices:\", InstanceOnTimes * inferentia_price )\n",
    "print(\"Lambda Prices:\", lambda_per_price * LambdaWorkers)\n",
    "print(\"Total Prices:\", lambda_per_price * LambdaWorkers + InstanceOnTimes * inferentia_price)\n",
    "\n",
    "print(\"Only Lambda Prices\", lambda_per_price * totals * 10)\n",
    "print(\"Only Inferentia Prices\", 600 * inferentia_price * MaxInstances )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}